{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.7.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.utils import \\\n",
    "    time_synchronized,select_device, increment_path,\\\n",
    "    scale_coords,xyxy2xywh,non_max_suppression,split_for_trace_model,\\\n",
    "    driving_area_mask,lane_line_mask,plot_one_box,show_seg_result,\\\n",
    "    AverageMeter,\\\n",
    "    LoadImages,\\\n",
    "    letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "\n",
    "world = client.get_world()\n",
    "spectator = world.get_spectator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_spectator_to(transform, distance=5.0, x=0, y=0, z=4, yaw=0, pitch=-30, roll=0):\n",
    "    back_location = transform.location - transform.get_forward_vector() * distance\n",
    "    \n",
    "    back_location.x += x\n",
    "    back_location.y += y\n",
    "    back_location.z += z\n",
    "    transform.rotation.yaw += yaw\n",
    "    transform.rotation.pitch = pitch\n",
    "    transform.rotation.roll = roll\n",
    "    \n",
    "    spectator_transform = carla.Transform(back_location, transform.rotation)\n",
    "    \n",
    "    spectator.set_transform(spectator_transform)\n",
    "\n",
    "def spawn_vehicle(vehicle_index=0, spawn_index=0, pattern='vehicle.*'):\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_bp = blueprint_library.filter(pattern)[vehicle_index]\n",
    "    spawn_point = world.get_map().get_spawn_points()[spawn_index]\n",
    "    vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "    return vehicle\n",
    "\n",
    "def draw_on_screen(world, transform, content='O', color=carla.Color(0, 255, 0), life_time=20):\n",
    "    world.debug.draw_string(transform.location, content, color=color, life_time=life_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=800, height=600):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.semantic_segmentation')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((600, 800, 4), dtype=np.uint8)\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    \n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "cv2.namedWindow('Semantic Segmentation Camera', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        cv2.imshow('Semantic segmentation Camera', video_output)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        # HARDCODED SETTINGS\n",
    "        weights = 'data/weights/yolopv2.pt'\n",
    "        imgsz = 640\n",
    "        device = '0'\n",
    "        classes = None\n",
    "        agnostic = False\n",
    "        conf_thres = 0.3\n",
    "        iou_thres=0.45\n",
    "\n",
    "        # Load model\n",
    "        stride = 32\n",
    "        model  = torch.jit.load(weights)\n",
    "        device = select_device(device)\n",
    "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "        model = model.to(device)\n",
    "\n",
    "        if half:\n",
    "            model.half()  # to FP16  \n",
    "        model.eval()\n",
    "\n",
    "        # Run inference\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "        random_bgr = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "        img0 = cv2.resize(random_bgr, (1280,720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred,anchor_grid],seg,ll= model(img)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # waste time: the incompatibility of  torch.jit.trace causes extra time consumption in demo version \n",
    "        # but this problem will not appear in offical version \n",
    "        tw1 = time_synchronized()\n",
    "        pred = split_for_trace_model(pred,anchor_grid)\n",
    "        tw2 = time_synchronized()\n",
    "\n",
    "        # Apply NMS\n",
    "        t3 = time_synchronized()\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic)\n",
    "        t4 = time_synchronized()\n",
    "\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "\n",
    "        show_seg_result(img0, (da_seg_mask,ll_seg_mask), is_demo=True)\n",
    "        return img0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    global video_output_seg\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    image_to_analyze = video_output[:, :, :3]\n",
    "    image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "    video_output_seg = detect(image_to_analyze)\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "#cv2.namedWindow('RGB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "#        cv2.imshow('RGB Camera', video_output)\n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
