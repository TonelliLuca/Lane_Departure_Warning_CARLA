{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.7.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.utils import \\\n",
    "    time_synchronized,select_device, increment_path,\\\n",
    "    scale_coords,xyxy2xywh,non_max_suppression,split_for_trace_model,\\\n",
    "    driving_area_mask,lane_line_mask,plot_one_box,show_seg_result,\\\n",
    "    AverageMeter,\\\n",
    "    LoadImages,\\\n",
    "    letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "\n",
    "world = client.get_world()\n",
    "spectator = world.get_spectator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_spectator_to(transform, distance=5.0, x=0, y=0, z=4, yaw=0, pitch=-30, roll=0):\n",
    "    back_location = transform.location - transform.get_forward_vector() * distance\n",
    "    \n",
    "    back_location.x += x\n",
    "    back_location.y += y\n",
    "    back_location.z += z\n",
    "    transform.rotation.yaw += yaw\n",
    "    transform.rotation.pitch = pitch\n",
    "    transform.rotation.roll = roll\n",
    "    \n",
    "    spectator_transform = carla.Transform(back_location, transform.rotation)\n",
    "    \n",
    "    spectator.set_transform(spectator_transform)\n",
    "\n",
    "def spawn_vehicle(vehicle_index=0, spawn_index=0, pattern='vehicle.*'):\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_bp = blueprint_library.filter(pattern)[vehicle_index]\n",
    "    spawn_point = world.get_map().get_spawn_points()[spawn_index]\n",
    "    vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "    return vehicle\n",
    "\n",
    "def draw_on_screen(world, transform, content='O', color=carla.Color(0, 255, 0), life_time=20):\n",
    "    world.debug.draw_string(transform.location, content, color=color, life_time=life_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=800, height=600):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.semantic_segmentation')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((600, 800, 4), dtype=np.uint8)\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    \n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "cv2.namedWindow('Semantic Segmentation Camera', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        cv2.imshow('Semantic segmentation Camera', video_output)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        # HARDCODED SETTINGS\n",
    "        weights = 'data/weights/yolopv2.pt'\n",
    "        imgsz = 640\n",
    "        device = '0'\n",
    "        classes = None\n",
    "        agnostic = False\n",
    "        conf_thres = 0.3\n",
    "        iou_thres=0.45\n",
    "\n",
    "        # Load model\n",
    "        stride = 32\n",
    "        model  = torch.jit.load(weights)\n",
    "        device = select_device(device)\n",
    "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "        model = model.to(device)\n",
    "\n",
    "        if half:\n",
    "            model.half()  # to FP16  \n",
    "        model.eval()\n",
    "\n",
    "        # Run inference\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "        random_bgr = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "        img0 = cv2.resize(random_bgr, (1280,720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred,anchor_grid],seg,ll= model(img)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # waste time: the incompatibility of  torch.jit.trace causes extra time consumption in demo version \n",
    "        # but this problem will not appear in offical version \n",
    "        tw1 = time_synchronized()\n",
    "        pred = split_for_trace_model(pred,anchor_grid)\n",
    "        tw2 = time_synchronized()\n",
    "\n",
    "        # Apply NMS\n",
    "        t3 = time_synchronized()\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic)\n",
    "        t4 = time_synchronized()\n",
    "\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "\n",
    "        show_seg_result(img0, (da_seg_mask,ll_seg_mask), is_demo=True)\n",
    "        return img0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spawn_vehicle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9448\\1830413920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcamera\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mvehicle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspawn_vehicle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mcamera\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspawn_camera\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattach_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spawn_vehicle' is not defined"
     ]
    }
   ],
   "source": [
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    global video_output_seg\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    image_to_analyze = video_output[:, :, :3]\n",
    "    image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "    video_output_seg = detect(image_to_analyze)\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "#cv2.namedWindow('RGB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "#        cv2.imshow('RGB Camera', video_output)\n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.utils import (\n",
    "    time_synchronized, select_device, increment_path,\n",
    "    scale_coords, xyxy2xywh, non_max_suppression, split_for_trace_model,\n",
    "    driving_area_mask, lane_line_mask, plot_one_box, show_seg_result,\n",
    "    AverageMeter, LoadImages, letterbox\n",
    ")\n",
    "from threading import Thread\n",
    "\n",
    "# Load the model once\n",
    "weights = 'data/weights/yolopv2.pt'\n",
    "imgsz = 640\n",
    "device = select_device('0')\n",
    "model = torch.jit.load(weights).to(device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "model.eval()\n",
    "\n",
    "# Run inference once to warm up the model\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        random_bgr = np.transpose(image, (1, 2, 0))\n",
    "        img0 = cv2.resize(random_bgr, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred, anchor_grid], seg, ll = model(img)\n",
    "        t2 = time_synchronized()\n",
    "        # Apply NMS\n",
    "        pred = split_for_trace_model(pred, anchor_grid)\n",
    "        pred = non_max_suppression(pred, 0.3, 0.45, classes=None, agnostic=False)\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "        show_seg_result(img0, (da_seg_mask, ll_seg_mask), is_demo=True)\n",
    "        return img0\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    global video_output_seg\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    image_to_analyze = video_output[:, :, :3]\n",
    "    image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "    video_output_seg = detect(image_to_analyze)\n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution provided with threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CROSSING to the LEFT lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n",
      "Car is CENTERED in the lane\n"
     ]
    }
   ],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.utils import (\n",
    "    time_synchronized, select_device, split_for_trace_model,\n",
    "    non_max_suppression, driving_area_mask, lane_line_mask,\n",
    "    show_seg_result, letterbox\n",
    ")\n",
    "from threading import Thread\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((800, 600))\n",
    "\n",
    "# Load the model once\n",
    "weights = 'data/weights/yolopv2.pt'\n",
    "imgsz = 640\n",
    "device = select_device('0')\n",
    "model = torch.jit.load(weights).to(device)\n",
    "half = device.type != 'cpu'  # Half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()  # Convert to FP16\n",
    "model.eval()\n",
    "\n",
    "# Warm up model\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the image\n",
    "        random_bgr = np.transpose(image, (1, 2, 0))\n",
    "        img0 = cv2.resize(random_bgr, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # Convert to float16/32\n",
    "        img /= 255.0  # Normalize to 0-1\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred, anchor_grid], seg, ll = model(img)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply segmentation masks\n",
    "        pred = split_for_trace_model(pred, anchor_grid)\n",
    "        pred = non_max_suppression(pred, 0.3, 0.45, classes=None, agnostic=False)\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "        \n",
    "        # Overlay segmentation masks on the original image\n",
    "        mask = np.zeros_like(img0, dtype=np.uint8)\n",
    "        mask[da_seg_mask == 1] = (0, 255, 0)  # Green for drivable area\n",
    "        mask[ll_seg_mask == 1] = (0, 0, 255)  # Red for lane lines\n",
    "        \n",
    "        # Merge segmentation mask with the original image\n",
    "        combined = cv2.addWeighted(img0, 0.7, mask, 0.3, 0)\n",
    "\n",
    "        # Process the lane line mask for red line detection\n",
    "        red_lane_mask = cv2.inRange(ll_seg_mask, 1, 255)\n",
    "\n",
    "        # Clean up noise with morphological operationsq\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        red_lane_mask = cv2.morphologyEx(red_lane_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours of red lines\n",
    "        contours, _ = cv2.findContours(red_lane_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw bounding boxes and alignment information\n",
    "        red_boxes = [cv2.boundingRect(cnt) for cnt in contours]\n",
    "        for (x, y, w, h) in red_boxes:\n",
    "            cv2.rectangle(combined, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box\n",
    "\n",
    "        # Lane alignment analysis\n",
    "        if red_boxes:\n",
    "            leftmost_red = min([x for x, y, w, h in red_boxes])\n",
    "            rightmost_red = max([x + w for x, y, w, h in red_boxes])\n",
    "            lane_center_x = (leftmost_red + rightmost_red) // 2\n",
    "            img_center_x = combined.shape[1] // 2\n",
    "\n",
    "            # Alignment status\n",
    "            if abs(lane_center_x - img_center_x) < 20:\n",
    "                alignment_status = \"Car is CENTERED in the lane\"\n",
    "                print(\"Car is CENTERED in the lane\")\n",
    "            elif lane_center_x > img_center_x:\n",
    "                alignment_status = \"Car is CROSSING to the LEFT lane\"\n",
    "                print(\"Car is CROSSING to the LEFT lane\")\n",
    "            else:\n",
    "                alignment_status = \"Car is CROSSING to the RIGHT lane\"\n",
    "                print(\"Car is CROSSING to the RIGHT lane\")\n",
    "\n",
    "            # Draw alignment lines and text\n",
    "            cv2.line(combined, (lane_center_x, 0), (lane_center_x, combined.shape[0]), (0, 0, 255), 2)\n",
    "            cv2.line(combined, (img_center_x, 0), (img_center_x, combined.shape[0]), (255, 0, 0), 2)\n",
    "            cv2.putText(combined, alignment_status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display segmentation mask for debugging\n",
    "        cv2.imshow(\"Lane Line Mask\", red_lane_mask)\n",
    "\n",
    "        return combined\n",
    "\n",
    "\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "counter = 2\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "def camera_callback(image):\n",
    "    global counter\n",
    "    global video_output\n",
    "    global video_output_seg\n",
    "    counter -= 1\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    if counter == 0:\n",
    "        image_to_analyze = video_output[:, :, :3]\n",
    "        image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "        Thread(target=run_inference, args=(image_to_analyze,)).start()\n",
    "\n",
    "def run_inference(image_to_analyze):\n",
    "    global counter\n",
    "    global video_output_seg\n",
    "    if counter == 0:\n",
    "        video_output_seg = detect(image_to_analyze)\n",
    "        counter = 2\n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(False)\n",
    "\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_w:\n",
    "                    vehicle.apply_control(carla.VehicleControl(throttle=1.0))\n",
    "                elif event.key == pygame.K_s:\n",
    "                    vehicle.apply_control(carla.VehicleControl(brake=1.0))\n",
    "                elif event.key == pygame.K_a:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=-1.0))\n",
    "                elif event.key == pygame.K_d:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=1.0))\n",
    "            elif event.type == pygame.KEYUP:\n",
    "                if event.key in [pygame.K_w, pygame.K_s]:\n",
    "                    vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "                elif event.key in [pygame.K_a, pygame.K_d]:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=0.0))\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.utils import (\n",
    "    time_synchronized, select_device, increment_path,\n",
    "    scale_coords, xyxy2xywh, non_max_suppression, split_for_trace_model,\n",
    "    driving_area_mask, lane_line_mask, plot_one_box, show_seg_result,\n",
    "    AverageMeter, LoadImages, letterbox\n",
    ")\n",
    "from threading import Thread\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((800, 600))\n",
    "\n",
    "# Load the model once\n",
    "weights = 'data/weights/yolopv2.pt'\n",
    "imgsz = 640\n",
    "device = select_device('0')\n",
    "model = torch.jit.load(weights).to(device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "model.eval()\n",
    "\n",
    "# Run inference once to warm up the model\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        # Original preprocessing and inference\n",
    "        random_bgr = np.transpose(image, (1, 2, 0))\n",
    "        img0 = cv2.resize(random_bgr, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        \n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred, anchor_grid], seg, ll = model(img)\n",
    "        t2 = time_synchronized()\n",
    "        \n",
    "        # Apply NMS\n",
    "        pred = split_for_trace_model(pred, anchor_grid)\n",
    "        pred = non_max_suppression(pred, 0.3, 0.45, classes=None, agnostic=False)\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "        \n",
    "        # Visualize segmentation result\n",
    "        show_seg_result(img0, (da_seg_mask, ll_seg_mask), is_demo=True)\n",
    "\n",
    "        # NEW: Lane center calculation\n",
    "        # Extract red lane markings from the lane line mask\n",
    "        hsv = cv2.cvtColor(ll_seg_mask, cv2.COLOR_BGR2HSV)\n",
    "        lower_red = np.array([0, 170, 110])  # Red lane threshold\n",
    "        upper_red = np.array([10, 255, 255])\n",
    "        red_mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        \n",
    "        # Find contours of red lane markings\n",
    "        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        red_boxes = [cv2.boundingRect(cnt) for cnt in contours]\n",
    "        \n",
    "        # Draw bounding boxes on the original image\n",
    "        for (x, y, w, h) in red_boxes:\n",
    "            cv2.rectangle(img0, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate leftmost and rightmost lane positions\n",
    "        if red_boxes:\n",
    "            leftmost_red = min([x for x, y, w, h in red_boxes])  # Smallest x value\n",
    "            rightmost_red = max([x + w for x, y, w, h in red_boxes])  # Largest x+w value\n",
    "\n",
    "            # Calculate the center of the lane\n",
    "            lane_center_x = (leftmost_red + rightmost_red) // 2\n",
    "            img_center_x = img0.shape[1] // 2  # Car's center assumed at image center\n",
    "\n",
    "            # Determine alignment status\n",
    "            if abs(lane_center_x - img_center_x) < 20:  # Threshold for being centered\n",
    "                alignment_status = \"Car is CENTERED in the lane\"\n",
    "            elif lane_center_x > img_center_x:\n",
    "                alignment_status = \"Car is CROSSING to the LEFT lane\"\n",
    "            else:\n",
    "                alignment_status = \"Car is CROSSING to the RIGHT lane\"\n",
    "\n",
    "            print(alignment_status)\n",
    "            \n",
    "            # Draw feedback on the image\n",
    "            cv2.line(img0, (lane_center_x, 0), (lane_center_x, img0.shape[0]), (0, 0, 255), 2)\n",
    "            cv2.line(img0, (img_center_x, 0), (img_center_x, img0.shape[0]), (255, 0, 0), 2)\n",
    "            cv2.putText(img0, alignment_status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Print the analysis mask\n",
    "        cv2.imshow('Lane Analysis Mask', red_mask)\n",
    "\n",
    "        return img0\n",
    "\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "counter = 2\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "def camera_callback(image):\n",
    "    global counter\n",
    "    global video_output\n",
    "    global video_output_seg\n",
    "    counter = counter -1\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    if counter == 0:\n",
    "        print(\"analyzing\")\n",
    "        image_to_analyze = video_output[:, :, :3]\n",
    "        image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "        # Start a new thread for the detect function\n",
    "        Thread(target=run_inference, args=(image_to_analyze,)).start()\n",
    "\n",
    "def run_inference(image_to_analyze):\n",
    "    global counter\n",
    "    global video_output_seg\n",
    "    if counter == 0:\n",
    "        video_output_seg = detect(image_to_analyze)\n",
    "        #cv2.imwrite('output_image.png', video_output_seg)  # Save the image to disk\n",
    "        counter = 2\n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(False)\n",
    "\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_w:\n",
    "                    vehicle.apply_control(carla.VehicleControl(throttle=1.0))\n",
    "                elif event.key == pygame.K_s:\n",
    "                    vehicle.apply_control(carla.VehicleControl(brake=1.0))\n",
    "                elif event.key == pygame.K_a:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=-1.0))\n",
    "                elif event.key == pygame.K_d:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=1.0))\n",
    "            elif event.type == pygame.KEYUP:\n",
    "                if event.key == pygame.K_w or event.key == pygame.K_s:\n",
    "                    vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "                elif event key == pygame.K_a or event.key == pygame.K_d:\n",
    "                    vehicle.apply_control(carla.VehicleControl(steer=0.0))\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\carla-env\\lib\\site-packages\\torch\\nn\\functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.utils import (\n",
    "    time_synchronized, select_device, increment_path,\n",
    "    scale_coords, xyxy2xywh, non_max_suppression, split_for_trace_model,\n",
    "    driving_area_mask, lane_line_mask, plot_one_box, show_seg_result,\n",
    "    AverageMeter, LoadImages, letterbox\n",
    ")\n",
    "from threading import Thread\n",
    "\n",
    "# Load the model once\n",
    "weights = 'data/weights/yolopv2.pt'\n",
    "imgsz = 640\n",
    "device = select_device('0')\n",
    "model = torch.jit.load(weights).to(device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "model.eval()\n",
    "\n",
    "# Run inference once to warm up the model\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "def detect(image):\n",
    "    with torch.no_grad():\n",
    "        # Crop the image to only the bottom part\n",
    "        height, width = image.shape[1], image.shape[2]\n",
    "        crop_height = height // 2  # Adjust this value to change the crop size\n",
    "        cropped_image = image[:, crop_height:, :]  # Crop the bottom half of the image\n",
    "\n",
    "        random_bgr = np.transpose(cropped_image, (1, 2, 0))\n",
    "        img0 = cv2.resize(random_bgr, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "        img = letterbox(img0)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        _, seg, ll = model(img)\n",
    "        t2 = time_synchronized()\n",
    "        # Driving area and lane detection\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "        show_seg_result(img0, (da_seg_mask, ll_seg_mask), is_demo=True)\n",
    "        return img0\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=2), carla.Rotation(pitch=-10)), width=640, height=640):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera\n",
    "\n",
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((640, 640, 4), dtype=np.uint8)\n",
    "video_output_seg = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    image_to_analyze = video_output[:, :, :3]\n",
    "    image_to_analyze = np.transpose(image_to_analyze, (2, 0, 1))\n",
    "    # Start a new thread for the detect function\n",
    "    Thread(target=run_inference, args=(image_to_analyze,)).start()\n",
    "\n",
    "def run_inference(image_to_analyze):\n",
    "    global video_output_seg\n",
    "    video_output_seg = detect(image_to_analyze)\n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "cv2.namedWindow('RGB analyzed output', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        cv2.imshow('RGB analyzed output', video_output_seg)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
